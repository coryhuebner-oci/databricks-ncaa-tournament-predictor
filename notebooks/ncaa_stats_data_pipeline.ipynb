{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCAA Stats Data Pipeline\n",
    "\n",
    "This \"pipeline\" is a notebook used to setup NCAA data in our Databricks sandbox. It's largely used as a workaround since we don't have access to DLT/jobs in our sandbox environment; For now, I'll just run the scripts manually like a peasant, but in real-life this could be converted to\n",
    "DLT pipelines, jobs, etc\n",
    "\n",
    "The steps in this notebook:\n",
    "1. Setup the initial schema for landing NCAA data\n",
    "1. Load raw data into Databricks\n",
    "1. Run ETL scripts to cleanup and transform data into a format suitable for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run cells in this section to get your environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup module autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables using dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session for the Databricks compute environment\n",
    "from pyspark.sql import SparkSession\n",
    "from ncaa_tournament_predictor.config import Config\n",
    "from ncaa_tournament_predictor.databricks import get_databricks_spark_session\n",
    "\n",
    "# Explicit typing as SparkSession here to help out intellisense...DatabricksSession intellisense\n",
    "# isn't very good. In all my exploration so far, the DatabricksSession is compatible with the SparkSession\n",
    "spark: SparkSession = get_databricks_spark_session(Config.databricks_profile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all cells above this one to setup your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Setup\n",
    "\n",
    "Initial steps to create a Databricks schema for holding NCAA mens basketball data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ncaa_mens_basketball schema\n",
    "spark.sql(\"create schema if not exists object_computing.ncaa_mens_basketball;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Volumes\n",
    "Setup volumes for holding raw data files from various external data sources (CSVs, text files, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a volume for raw Kaggle stats data\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "raw_kaggle_stats_sql_object = volumes.as_sql_object(volumes.raw_kaggle_stats)\n",
    "spark.sql(f\"create volume if not exists {raw_kaggle_stats_sql_object}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy raw data into the raw_kaggle_stats volume\n",
    "\n",
    "import os\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "notebook_dir = os.path.abspath(os.getcwd())\n",
    "kaggle_dataset_path = os.path.abspath(\n",
    "    os.path.join(notebook_dir, \"../datasets/kaggle_ncaa_stats\")\n",
    ")\n",
    "\n",
    "for filename in os.listdir(kaggle_dataset_path):\n",
    "    spark.copyFromLocalToFs(\n",
    "        local_path=os.path.join(kaggle_dataset_path, filename),\n",
    "        dest_path=os.path.join(volumes.without_dbfs_protocol(volumes.raw_kaggle_stats), filename)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Kaggle stats dataset\n",
    "from ncaa_tournament_predictor import transformation, volumes\n",
    "\n",
    "raw_kaggle_stats = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .options(header=True, inferSchema=True, mergeSchema=True)\n",
    "        .load(volumes.raw_kaggle_stats)\n",
    ")\n",
    "cleaned_ncaa_data = transformation.get_cleaned_kaggle_stats(raw_kaggle_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a volume for raw head-to-head data\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "spark.sql(f\"create volume if not exists {volumes.as_sql_object(volumes.raw_head_to_head)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy raw data into the raw_head_to_head volume\n",
    "\n",
    "import os\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "notebook_dir = os.path.abspath(os.getcwd())\n",
    "head_to_head_dataset_path = os.path.abspath(\n",
    "    os.path.join(notebook_dir, \"../datasets/kenpom_head_to_head\")\n",
    ")\n",
    "\n",
    "for filename in os.listdir(head_to_head_dataset_path):\n",
    "    spark.copyFromLocalToFs(\n",
    "        local_path=os.path.join(head_to_head_dataset_path, filename),\n",
    "        dest_path=os.path.join(volumes.without_dbfs_protocol(volumes.raw_head_to_head), filename)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup & Transformation\n",
    "Process the raw data, clean it up, and transform it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964a03c9e45d4e1f9991e550d7cf0aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ea8a5385f545c6bb008f5cabc0220f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efee9f7fba8643baa18dcb186e363f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b1e808ce8249ef855e5e3ea8e16b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d1ffd460c543989b58d24f640777b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/27 09:11:21 INFO databricks.ml_features._compute_client._compute_client: Setting columns ['team', 'college_season'] of table 'object_computing.ncaa_mens_basketball.game_prediction_features' to NOT NULL.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c31f955d97b427080f86dc5d9b11f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8713acb7cf440e7b6dcb21eae7d3dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/27 09:11:27 INFO databricks.ml_features._compute_client._compute_client: Setting Primary Keys constraint ['team', 'college_season'] on table 'object_computing.ncaa_mens_basketball.game_prediction_features'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de84695522c4d43a4639151a8e6f799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "{'error_code': 401, 'message': 'Credential was not sent or was of an unsupported type for this API. [ReqId: 1ad914ab-1373-49ac-af5a-48096d9c875b]'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create the cleaned Kaggle datasets table\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mncaa_tournament_predictor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjobs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m kaggle_stats\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mkaggle_stats\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_cleaned_kaggle_stats_to_feature_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36mconvert_cleaned_kaggle_stats_to_feature_store\u001b[39m\u001b[34m(spark)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/feature_engineering/client.py:357\u001b[39m, in \u001b[36mFeatureEngineeringClient.create_table\u001b[39m\u001b[34m(self, name, primary_keys, df, timeseries_column, partition_columns, schema, description, tags, **kwargs)\u001b[39m\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    347\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimeseries column \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeseries_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not in primary_keys. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    348\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimeseries columns must be primary keys.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    349\u001b[39m     )\n\u001b[32m    351\u001b[39m name = uc_utils.get_full_table_name(\n\u001b[32m    352\u001b[39m     name,\n\u001b[32m    353\u001b[39m     \u001b[38;5;28mself\u001b[39m._spark_client.get_current_catalog(),\n\u001b[32m    354\u001b[39m     \u001b[38;5;28mself\u001b[39m._spark_client.get_current_database(),\n\u001b[32m    355\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestamp_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeseries_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFEATURE_ENGINEERING_CLIENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/_compute_client/_compute_client.py:108\u001b[39m, in \u001b[36mComputeClient.create_table\u001b[39m\u001b[34m(self, name, primary_keys, df, timestamp_keys, partition_columns, schema, description, tags, client_name, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPath argument is not supported for Unity Catalog tables.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    106\u001b[39m validation_utils.check_kwargs_empty(kwargs, \u001b[33m\"\u001b[39m\u001b[33mcreate_table\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestamp_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamp_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRequestContext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCREATE_TABLE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_name\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/_compute_client/_compute_client.py:281\u001b[39m, in \u001b[36mComputeClient._create_table\u001b[39m\u001b[34m(self, name, primary_keys, df, timestamp_keys, partition_columns, schema, description, path, tags, req_context)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# Delete empty Delta table.  The feature table will have already been cleaned up from the catalog.\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m._spark_client.delete_empty_table(name)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# 4. Write to Delta table\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/_compute_client/_compute_client.py:267\u001b[39m, in \u001b[36mComputeClient._create_table\u001b[39m\u001b[34m(self, name, primary_keys, df, timestamp_keys, partition_columns, schema, description, path, tags, req_context)\u001b[39m\n\u001b[32m    259\u001b[39m feature_key_specs = \u001b[38;5;28mself\u001b[39m._get_feature_key_specs(\n\u001b[32m    260\u001b[39m     delta_schema,\n\u001b[32m    261\u001b[39m     primary_keys_as_list,\n\u001b[32m    262\u001b[39m     timestamp_keys_as_list,\n\u001b[32m    263\u001b[39m     partition_cols_as_list,\n\u001b[32m    264\u001b[39m )\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_feature_table_with_features_and_tags\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartition_key_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_key_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprimary_key_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_key_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestamp_key_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamp_key_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_uc_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_imported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_key_specs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_key_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# Delete empty Delta table.  The feature table will have already been cleaned up from the catalog.\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m._spark_client.delete_empty_table(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/_compute_client/_compute_client.py:1151\u001b[39m, in \u001b[36mComputeClient._create_feature_table_with_features_and_tags\u001b[39m\u001b[34m(self, name, partition_key_specs, primary_key_specs, timestamp_key_specs, feature_key_specs, is_imported, tags, description, req_context)\u001b[39m\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_table:\n\u001b[32m   1150\u001b[39m     \u001b[38;5;28mself\u001b[39m._catalog_client.delete_feature_table(name, req_context)\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/_compute_client/_compute_client.py:1125\u001b[39m, in \u001b[36mComputeClient._create_feature_table_with_features_and_tags\u001b[39m\u001b[34m(self, name, partition_key_specs, primary_key_specs, timestamp_key_specs, feature_key_specs, is_imported, tags, description, req_context)\u001b[39m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1123\u001b[39m     create_feature_table_req_context = req_context\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m feature_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_catalog_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_feature_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_key_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_key_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimary_key_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_key_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestamp_key_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamp_key_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_imported\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_imported\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_feature_table_req_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feature_key_specs) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_uc_table:\n\u001b[32m   1135\u001b[39m     \u001b[38;5;28mself\u001b[39m._catalog_client.create_features(\n\u001b[32m   1136\u001b[39m         name, feature_key_specs, req_context\n\u001b[32m   1137\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/_catalog_client/_catalog_client.py:223\u001b[39m, in \u001b[36mCatalogClient.create_feature_table\u001b[39m\u001b[34m(self, feature_table, partition_key_spec, primary_key_spec, timestamp_key_spec, description, is_imported, req_context)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_feature_table\u001b[39m(\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    207\u001b[39m     feature_table: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m     req_context: RequestContext,\n\u001b[32m    214\u001b[39m ):\n\u001b[32m    215\u001b[39m     req_body = CreateFeatureTable(\n\u001b[32m    216\u001b[39m         name=reformat_full_table_name(feature_table),\n\u001b[32m    217\u001b[39m         primary_keys=([key_spec.to_proto() \u001b[38;5;28;01mfor\u001b[39;00m key_spec \u001b[38;5;129;01min\u001b[39;00m primary_key_spec]),\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m         is_imported=is_imported,\n\u001b[32m    222\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCreateFeatureTable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FeatureTable.from_proto(response_proto.feature_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/_catalog_client/_catalog_client.py:185\u001b[39m, in \u001b[36mCatalogClient._call_endpoint\u001b[39m\u001b[34m(self, api, proto, req_context)\u001b[39m\n\u001b[32m    183\u001b[39m resolved_endpoint = \u001b[38;5;28mself\u001b[39m._resolve_url_params(endpoint, request_dict)\n\u001b[32m    184\u001b[39m response_proto = api.Response()\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/utils/rest_utils.py:280\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, req_context, timeout)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    272\u001b[39m     response = http_request(\n\u001b[32m    273\u001b[39m         host_creds=host_creds,\n\u001b[32m    274\u001b[39m         endpoint=endpoint,\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m         timeout=timeout,\n\u001b[32m    279\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m response = \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m js_dict = json.loads(response.text)\n\u001b[32m    282\u001b[39m json_to_proto(js_dict=js_dict, message=response_proto)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/learning/databricks-ncaa-tournament-predictor/.venvs/databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster/lib/python3.12/site-packages/databricks/ml_features/utils/rest_utils.py:165\u001b[39m, in \u001b[36mverify_rest_response\u001b[39m\u001b[34m(response, endpoint)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _can_parse_as_json(response.text):\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# ToDo(ML-20622): return cleaner error to client, eg: mlflow.exceptions.RestException\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(json.loads(response.text))\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    167\u001b[39m         base_msg = (\n\u001b[32m    168\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m failed with error code \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    169\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m != 200\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    173\u001b[39m             )\n\u001b[32m    174\u001b[39m         )\n",
      "\u001b[31mException\u001b[39m: {'error_code': 401, 'message': 'Credential was not sent or was of an unsupported type for this API. [ReqId: 1ad914ab-1373-49ac-af5a-48096d9c875b]'}"
     ]
    }
   ],
   "source": [
    "# Create the cleaned Kaggle datasets table\n",
    "from ncaa_tournament_predictor.jobs import kaggle_stats\n",
    "\n",
    "kaggle_stats.convert_cleaned_kaggle_stats_to_feature_store(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c2a8ac258a4d669f5e1bac85e70043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the cleaned head-to-head table\n",
    "\n",
    "from ncaa_tournament_predictor.jobs import head_to_head\n",
    "\n",
    "head_to_head.run_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training Datasets\n",
    "Combine data sets to create a dataset used for training an ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68312"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the \n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "from ncaa_tournament_predictor import transformation, tables\n",
    "\n",
    "\n",
    "team_stats = spark.read.table(tables.cleaned_kaggle_stats)\n",
    "head_to_head_results = spark.read.table(tables.cleaned_head_to_head_results)\n",
    "\n",
    "training_dataset = transformation.get_training_dataset(team_stats, head_to_head_results)\n",
    "training_dataset_sample = training_dataset.orderBy(rand()).limit(500)\n",
    "training_dataset.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

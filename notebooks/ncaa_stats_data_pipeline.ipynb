{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCAA Stats Data Pipeline\n",
    "\n",
    "This \"pipeline\" is a notebook used to setup NCAA data in our Databricks sandbox. It's largely used as a workaround since we don't have access to DLT/jobs in our sandbox environment; For now, I'll just run the scripts manually like a peasant, but in real-life this could be converted to\n",
    "DLT pipelines, jobs, etc\n",
    "\n",
    "The steps in this notebook:\n",
    "1. Setup the initial schema for landing NCAA data\n",
    "1. Load raw data into Databricks\n",
    "1. Run ETL scripts to cleanup and transform data into a format suitable for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run cells in this section to get your environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup module autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables using dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session for the Databricks compute environment\n",
    "from pyspark.sql import SparkSession\n",
    "from ncaa_tournament_predictor.config import Config\n",
    "from ncaa_tournament_predictor.databricks import get_databricks_spark_session\n",
    "\n",
    "# Explicit typing as SparkSession here to help out intellisense...DatabricksSession intellisense\n",
    "# isn't very good. In all my exploration so far, the DatabricksSession is compatible with the SparkSession\n",
    "spark: SparkSession = get_databricks_spark_session(Config.databricks_profile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all cells above this one to setup your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Setup\n",
    "\n",
    "Initial steps to create a Databricks schema for holding NCAA mens basketball data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ncaa_mens_basketball schema\n",
    "spark.sql(\"create schema if not exists object_computing.ncaa_mens_basketball;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Volumes\n",
    "Setup volumes for holding raw data files from various external data sources (CSVs, text files, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a volume for raw Kaggle stats data\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "raw_kaggle_stats_sql_object = volumes.as_sql_object(volumes.raw_kaggle_stats)\n",
    "spark.sql(f\"create volume if not exists {raw_kaggle_stats_sql_object}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy raw data into the raw_kaggle_stats volume\n",
    "\n",
    "import os\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "notebook_dir = os.path.abspath(os.getcwd())\n",
    "kaggle_dataset_path = os.path.abspath(\n",
    "    os.path.join(notebook_dir, \"../datasets/kaggle_ncaa_stats\")\n",
    ")\n",
    "\n",
    "for filename in os.listdir(kaggle_dataset_path):\n",
    "    spark.copyFromLocalToFs(\n",
    "        local_path=os.path.join(kaggle_dataset_path, filename),\n",
    "        dest_path=os.path.join(volumes.without_dbfs_protocol(volumes.raw_kaggle_stats), filename)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Kaggle stats dataset\n",
    "from ncaa_tournament_predictor import transformation, volumes\n",
    "\n",
    "raw_kaggle_stats = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .options(header=True, inferSchema=True, mergeSchema=True)\n",
    "        .load(volumes.raw_kaggle_stats)\n",
    ")\n",
    "cleaned_ncaa_data = transformation.get_cleaned_kaggle_stats(raw_kaggle_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a volume for raw head-to-head data\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "spark.sql(f\"create volume if not exists {volumes.as_sql_object(volumes.raw_head_to_head)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy raw data into the raw_head_to_head volume\n",
    "\n",
    "import os\n",
    "\n",
    "from ncaa_tournament_predictor import volumes\n",
    "\n",
    "notebook_dir = os.path.abspath(os.getcwd())\n",
    "head_to_head_dataset_path = os.path.abspath(\n",
    "    os.path.join(notebook_dir, \"../datasets/kenpom_head_to_head\")\n",
    ")\n",
    "\n",
    "for filename in os.listdir(head_to_head_dataset_path):\n",
    "    spark.copyFromLocalToFs(\n",
    "        local_path=os.path.join(head_to_head_dataset_path, filename),\n",
    "        dest_path=os.path.join(volumes.without_dbfs_protocol(volumes.raw_head_to_head), filename)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup & Transformation\n",
    "Process the raw data, clean it up, and transform it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cleaned Kaggle datasets table\n",
    "from ncaa_tournament_predictor.jobs import kaggle_stats\n",
    "\n",
    "kaggle_stats.run_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cleaned head-to-head table\n",
    "\n",
    "from ncaa_tournament_predictor.jobs import head_to_head\n",
    "\n",
    "head_to_head.run_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the team stats and head-to-head results to build a training dataset\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "from ncaa_tournament_predictor import transformation, tables\n",
    "\n",
    "\n",
    "team_stats = spark.read.table(tables.cleaned_kaggle_stats)\n",
    "head_to_head_results = spark.read.table(tables.cleaned_head_to_head_results)\n",
    "\n",
    "training_dataset = transformation.get_training_dataset(team_stats, head_to_head_results)\n",
    "training_dataset_sample = training_dataset.orderBy(rand()).limit(500)\n",
    "training_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training Datasets\n",
    "Combine data sets to create a dataset used for training an ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
      "array([ 31.    ,  19.    , 113.5   , 113.1   ,   0.5103,  57.9   ,\n",
      "        51.8   ,  15.3   ,  14.4   ,  24.1   ,  33.4   ,  28.7   ,\n",
      "        27.1   ,  54.7   ,  51.8   ,  40.9   ,  34.6   ,  71.3   ,\n",
      "        -5.1   ,      nan,  31.    ,  17.    , 102.9   , 102.4   ,\n",
      "         0.5136,  51.5   ,  47.7   ,  17.2   ,  15.9   ,  31.7   ,\n",
      "        27.5   ,  37.3   ,  35.3   ,  51.5   ,  48.3   ,  34.2   ,\n",
      "        31.2   ,  68.    ,  -8.4   ,  16.    ], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)\n"
     ]
    }
   ],
   "source": [
    "from ncaa_tournament_predictor.transformation import training_data_tensorflow\n",
    "\n",
    "training_data_tensor = training_data_tensorflow.to_tf_dataset(training_dataset_sample, team_stats)\n",
    "first_tensor = first_row = next(iter(training_data_tensor))\n",
    "print(first_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks-ncaa-tournament-predictor-Y0X8jGzL-remote-databricks-cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
